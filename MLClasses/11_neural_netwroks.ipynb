{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFdbqvS5A4Fx"
   },
   "source": [
    "### План тетрадки:\n",
    "- обзор\n",
    "    - что такое NN\n",
    "    - зачем NN\n",
    "    - что есть в NN\n",
    "    - как работает NN в целом\n",
    "    - типы NN\n",
    "- подробнее\n",
    "    - что такое нейрон\n",
    "    - связи и сдвиги\n",
    "    - обучение\n",
    "    - лосс\n",
    "    - градиентный спуск\n",
    "    - бэкпроп\n",
    "- ссылки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtm9goWsu7Kr"
   },
   "source": [
    "# Обзор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](linear-vs-nonlinear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порой попадаются данные, при работе с которыми классические алгоритмы машинного обучения не приносят результатов. Самое время обратить взгляд на нейросети!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZkdQuB9A4F-"
   },
   "source": [
    "## что такое нейросеть\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elPj_zoe6y3r"
   },
   "source": [
    "Искусственная нейронная сеть — это набор некоторых абстракций, которые мы будем называть \"нейроны\",  а еще -- связей между ними. Нейросеть можно рассматривать как сложную функцию (или набор связанных функций). \n",
    "\n",
    "Нейросеть часто представляется в виде такой визуализации: \n",
    "\n",
    "![](deep_neural_networks.png)\n",
    "<img width='600' src='https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6aa1OeRwUz_"
   },
   "source": [
    "## что есть в нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I8fdNgO641h"
   },
   "source": [
    "- нейроны, организованные в \"слои\"\n",
    "    - входной слой\n",
    "    - скрытый слой (их может быть несколько)\n",
    "    - выходной слой\n",
    "- связи между слоями\n",
    "- bias *(обычно этот термин не переводится)*\n",
    "\n",
    "![](nn_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7XmcP8vwVXm"
   },
   "source": [
    "## как работает нейросеть в целом\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_DW0T0fIA2g"
   },
   "source": [
    "- определяемся с данными\n",
    "- определяемся с архитектурой сетки (+ строим ее)\n",
    "- обучаем сетку\n",
    "- оцениваем результаты\n",
    "- повторяем предыдущие два пункта при необходимости\n",
    "- сохраняем обученную модель\n",
    "- используем по назначению\n",
    "\n",
    "\n",
    "**как происходит обучение:** <br>\n",
    "задача сети при обучении -- правильно расставить веса между слоями нейронов, чтобы нейроны реагировали на нужные сигналы.\n",
    "\n",
    "Сначала все веса просто расставлены случайно *(это называется инициализация весов)*, мы показываем сети данные, она выдаёт некоторый случайный ответ.<br>\n",
    "Затем мы сравниваем, насколько результат отличается от нужного нам *(считаем значение функции ошибки)*. Затем идём по сети в обратном направлении, от выходного слоя к его предшествующему, от того - к его предшествуннику и так до входного слоя.  На каждом из этих шагов  проверяем, как сработал каждый нейрон: какие связи привели к слишком большому значению нейрона(их стоит погасить), а какие наоборот стоит усилить.<br>\n",
    "Спустя большое количество таких циклов «прогнали-проверили ошибку-обновили веса» связи между нейронами в сети корректируются в сторону правильных. \n",
    "\n",
    "В больших сетях все это делается не вручную, а функциями.<br>\n",
    "На маленьких можно [посчитать руками](https://www.youtube.com/watch?v=khUVIZ3MON8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ufJZQjd6iES"
   },
   "source": [
    "## зачем/где применять нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_On1yah9NXQ"
   },
   "source": [
    "для задач вместо алгоритмов классического ML\n",
    "\n",
    "\n",
    "- аудио:\n",
    "    - распознавание и синтез речи\n",
    "    - классификация и генерация музыки\n",
    "- изображения\n",
    "    - определение объектов\n",
    "    - обработка изображений, перенос стиля\n",
    "    - классификация и генерация изображений\n",
    "- тексты\n",
    "    - машинный перевод,\n",
    "    - классификация текстов\n",
    "    - генерация текстов, языковые модели, перенос стиля\n",
    "\n",
    "\n",
    "Плюсы:\n",
    "- возможность решить практически любую задачу, которую можно формализовать ([как аппроксимацию функций](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A6%D1%8B%D0%B1%D0%B5%D0%BD%D0%BA%D0%BE))\n",
    "- более точные результаты предсказаний\n",
    "\n",
    "Минусы:\n",
    "- для обучения нужен большой объем данных и большие вычислительные мощности\n",
    "- нейросети работают медленнее, тк более \"громоздкие\", чем классические алгоритмы "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhp5EQAPwVs3"
   },
   "source": [
    "## типы сеток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRCeI9wQBAm4"
   },
   "source": [
    "Самые популярные:\n",
    "\n",
    "|архитектура|подтипы|задачи|\n",
    "|--|--|--|\n",
    "|[перцептроны](https://en.wikipedia.org/wiki/Multilayer_perceptron)||классификация,  регрессия|\n",
    "|[сверточные](https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C) (CNN)||обработка изображений|\n",
    "|[рекуррентные](https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C) (RNN)|[LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory), [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit)|обработка текста|\n",
    "|[адверсариальные](https://en.wikipedia.org/wiki/Generative_adversarial_network) (GAN)||генерация, перенос стиля|\n",
    "|[seq2seq](https://en.wikipedia.org/wiki/Seq2seq)|[автоэнкодеры](https://en.wikipedia.org/wiki/Autoencoder)<br>[трансформеры](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)|обработка текста (весь  NLP)\n",
    "\n",
    "\n",
    "а вот их [более полный лист](https://www.asimovinstitute.org/neural-network-zoo/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5CS8eIwAcAr"
   },
   "source": [
    "<img src=\"nn_types.png\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQnInLEru9nn"
   },
   "source": [
    "# Подробнее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHI2rv79A4GA"
   },
   "source": [
    "## что такое нейрон\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut3lSv8tMn8i"
   },
   "source": [
    "- Нейрон -- это абстракция. Нейрон можно представлять как функцию с кучей входов и одним выходом. Обычно результат работы этой функции существует на промежутке $[0,1]$<br> \n",
    "- Задача нейрона — взять числа от всех входов, выполнить над ними функцию и сохранить результат (это называется активацией нейрона).<br> \n",
    "- *Пример нейрона:* просуммировать все цифры со входов, и если их сумма больше N — выдать на выход единицу, иначе — ноль.\n",
    "\n",
    "- Слои нейросети состоят из нейронов со значениями их активации\n",
    "- на первом слое кол-во нейронов = кол-во признаков(features) у сэмпла данных\n",
    "\n",
    "<img src=\"neuron.jpeg\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEi_rVWSx46r"
   },
   "source": [
    "## связи и bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvCyEr-6oDFw"
   },
   "source": [
    "- Связи — это каналы, которыми соединены нейроны предыдущего и следующего слоя. У каждой связи есть свой вес (можно представить как важность связи).\n",
    "- *Пример:* когда через связь с весом 0.5 проходит число 10, оно превращается в 5. \n",
    "- Веса в связах нужны, чтобы управлять на какие входы нейрон должен реагировать, а на какие нет.\n",
    "- bias - дополнительный параметр, который корректирует значения финальной взвешенной суммы нейрона (он показывает, насколько значение финальной взвешенной суммы необходимо поднять/опустить, перед тем как нейрон активируется)\n",
    "\n",
    "<img src=\"nn_channels_bias.png\" width=\"400\"/>\n",
    "\n",
    "*(ссылки на картинки: [1](https://vas3k.ru/blog/machine_learning/), [2](https://towardsdatascience.com/coding-up-a-neural-network-classifier-from-scratch-977d235d8a24))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6JI-eSLwzdX"
   },
   "source": [
    "## активация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIRptqBFT51A"
   },
   "source": [
    "- активация -- это некоторое значение нейрона во время обучения, результат выполнения функции, заданной нейроном \n",
    "\n",
    "- во время обучения активация нейронов одного слоя влияет на активацию нейронов следующего\n",
    "- мы не можем напрямую задавать значения активаций, но можем влиять на веса связей между слоями\n",
    "- активации бывают [разными](http://rasbt.github.io/mlxtend/user_guide/general_concepts/activation-functions/)\n",
    "- почти все функции активации нелинейны. Это специально: нелинейность нужна, чтобы нейросеть в процессе обучения смогла провести нелинейную границу между классами обучающих данных **(именно позволяет нейросетям решать задачи с нелинейной зависимостью)**. Иначе получится регрессия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf69AmXJoKyw"
   },
   "source": [
    "## как записать слои и веса в матричном виде: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwG1OiCIw2aL"
   },
   "source": [
    "![](nn_forward_feed.png)\n",
    "\n",
    "**forward feed** \n",
    "\n",
    "- нейроны одного слоя (с их значениями) -- как вектор-столбец $a$\n",
    "\n",
    "\\begin{bmatrix}\n",
    "           a_{1} \\\\\n",
    "           a_{2} \\\\\n",
    "           a_{3}\n",
    "         \\end{bmatrix}\n",
    "<br>\n",
    "\n",
    "- веса -- как матрица $W$ , где строка -- это веса связей от нейронов одного слоя к конкретному нейрону следующего слоя\n",
    "\n",
    "\\begin{bmatrix}\n",
    "           w_{11}, w_{21}, w_{31} \\\\\n",
    "           w_{12}, w_{22}, w_{32} \\\\\n",
    "           w_{13}, w_{23}, w_{33} \\\\\n",
    "         \\end{bmatrix}\n",
    "<br>\n",
    "\n",
    "- не забываем про байесы --  $b$\n",
    "\n",
    "\\begin{bmatrix}\n",
    "           b_{1} \\\\\n",
    "           b_{2} \\\\\n",
    "           b_{3} \\\\\n",
    "         \\end{bmatrix}\n",
    "<br>\n",
    "\n",
    "- тогда финальная взвешенная сумма = $Wa+b$\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "           w_{11}, w_{21}, w_{31} \\\\\n",
    "           w_{12}, w_{22}, w_{32} \\\\\n",
    "           w_{13}, w_{23}, w_{33} \\\\\n",
    "         \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "           a_{1} \\\\\n",
    "           a_{2} \\\\\n",
    "           a_{3}\n",
    "         \\end{bmatrix}\n",
    "{} &+ \\begin{bmatrix}\n",
    "           b_{1} \\\\\n",
    "           b_{2} \\\\\n",
    "           b_{3} \\\\\n",
    "         \\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3} \\\\\n",
    "         \\end{bmatrix}\n",
    "\\end{align*}\n",
    "<br>\n",
    "\n",
    "- при перемножении $Wa$ получается новый вектор-столбец. Значения в нем --  это активации нейронов в новом слое \n",
    "    - если хотим, чтобы слой был нелинейным, обернем все проиведение в нелинейную функцию активации: $Out = f(Wa+b)$\n",
    "- один слой посчитан!\n",
    "    - следующие считаем по такому же принципу\n",
    "\n",
    "ок, вот так обучаемся. А как узнать, насколько мы ошиблись? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOhnsOkEw28X"
   },
   "source": [
    "## лосс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKmowgyKoObk"
   },
   "source": [
    "- ошибка -- это показатель, насколько сеть ошиблась в предсказании *(настоящий лейбл vs предсказанный лейбл)*.\n",
    "- Loss Function -- это метод посчитать такую ошибку.\n",
    "- при обучении модели, лосс-функция на вход принимает результат работы всех весов и байесов модели (которые выдали предсказание) и выдает число, описывающее, насколько велика ошибка\n",
    "- основная задача обучения -- минимизировать значение функции ошибки\n",
    "\n",
    "популярные типы loss-функций\n",
    "\n",
    "| название                            | где используется                |\n",
    "|-------------------------------------|---------------------------------|\n",
    "| Mean Squared Error (MSE)            | регрессия                       |\n",
    "| Binary Crossentropy (BCE)           | классификация (бинарная)        |\n",
    "| Maximum Likelihood Estimation (MLE) | классификация (мультиклассовая) |\n",
    "| KL-divergence                       | классификация                   |\n",
    "\n",
    "[как выбрать лосс](https://en.wikipedia.org/wiki/Loss_function#Selecting_a_loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZI2FfsTw3Xr"
   },
   "source": [
    "## градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9lc-MT3zyLx"
   },
   "source": [
    "Основная задача обучения -- минимизировать значение функции ошибки.\n",
    "Как минимизировать значение функции? <br>\n",
    "Рассмотрим на примере простой нелинейной:\n",
    "\n",
    "![](nn_gradient_descent.png)\n",
    "\n",
    "(чтобы найти минимумы/максимумы, нужно найти, в каких точках производная функции равна нулю)\n",
    "- [рекап про производные](https://function-x.ru/derivative.html)\n",
    "- [рекап про экстремумы функции](https://www.mathsisfun.com/calculus/maxima-minima.html)\n",
    "\n",
    "\n",
    "Функция, с которой нейросеть имеет дело, не только нелинейна, но и существует в многомерном пространстве. Поэтому понадобится не производная, а [градиент](https://ru.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82).\n",
    "\n",
    "- Если посчитать градиент функции ,  можно определить, в каком направлении (т.е. при каких параметрах) функция быстрее всего возрастает.\n",
    "    - длина вектора градиента показывает, насколько быстро функция растет в этом направлении\n",
    "     - чтобы минимизировать функцию, нужно двигаться ровно в противоположном направлении\n",
    "\n",
    "- \"Определить\" минимальные значения получается не сразу,  поэтому сеть учится в несколько ~~сотен,  иногда тысяч~~ заходов, начиная с рандомных параметров. Как определить минимум мультипараметрической функции:\n",
    " - посчитать градиент при рандомно заданных параметрах\n",
    " - сделать маленький шаг в направлении, противоположном градиенту\n",
    " - повторить предыдущие два шага\n",
    " - так пока не найдется минимум\n",
    "\n",
    "![](nn_local_min.gif)\n",
    "\n",
    "![](gradient-descent-local-minima.png)\n",
    "\n",
    "<img class=\"td-attachment-page-image\" src=\"https://i1.wp.com/bdtechtalks.com/wp-content/uploads/2020/04/gradient-descent-local-minima.png?fit=800%2C596&amp;ssl=1\" alt=\"hidden back door\">\n",
    "\n",
    "- [как посчитать градиент](https://www.matematicus.ru/vysshaya-matematika/proizvodnaya/gradient-funktsii-neskolkih-peremennyh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzRi9dhTBILR"
   },
   "source": [
    "\"нейросеть учится\" = \"нейросеть учится минимизировать функцию ошибки\" = учится делать предсказания более точными, изменяя веса связей между слоями (а также значения баесов).\n",
    "\n",
    "Как это происходит?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZNgwU_Iw37H"
   },
   "source": [
    "## обратное распространение ошибки (backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo53F-sU6gBK"
   },
   "source": [
    "backpropagation -- это метод посчитать градиент \n",
    "\n",
    "- градиент-вектор -- это вектор из компонентов по числу всех связей и баесов в сети, на всех слоях. Думать о нем в пространстве не очень удобно, но можно представить удобнее: некоторые компоненты (с большими позитивными или негативными значениями) -- это очень важные для обучения модели(т.е. для лосс-функции) параметры (связи между некоторыми нейронами или баесы).\n",
    "\n",
    "![](back-propagation.png)\n",
    "\n",
    "- чтобы минимизировать функцию ошибки, нужно повлиять на активацию нейронов последнего слоя. Чтобы повлиять на эту активацию, нужно изменить веса предыдущего слоя. И так далее, до самого начала.  Понять, в какую сторону их менять, помогает градиент лосс-функции. \n",
    "(нужно двигаться в противоположном направлении)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epWzX43QL2xF"
   },
   "source": [
    "**чуть больше математики:**\n",
    "\n",
    "чтобы определить каждую компоненту градиента,  нужно посчитать  производную сложной функции \n",
    "$pred = f(W_3* f(W_2* f(W_1a+b_1)+b_2)+b_3)$ , которой мы определили нейросеть\n",
    "\n",
    "идею backpropagation идеально объяснил Грант Сандерсон в своем [видео с примерами](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "\n",
    "<img src=\"nn_forwardpass_vs_backpass.png\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqcaDCTKQVUB"
   },
   "source": [
    "Обучение нейросети -- это последовательность циклов «прогнали-проверили ошибку-обновили веса связей». В результате этого связи между нейронами в сети корректируются в сторону правильных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxLO9Gu4t6VH"
   },
   "source": [
    "# Полезные ссылки\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvcU8c8bu3lW"
   },
   "source": [
    "**видео**\n",
    "\n",
    "1.  [про структуры нейросетей](https://youtu.be/aircAruvnKk)\n",
    "2.  [про обучение нейросетей](https://youtu.be/IHZwWFHWa-w)\n",
    "3. [backpropagation на простом примере](https://www.youtube.com/watch?v=khUVIZ3MON8)\n",
    "\n",
    "**статьи**\n",
    "1. [типы нейросетей](https://www.asimovinstitute.org/neural-network-zoo/)\n",
    "\n",
    "2. [таймлайн машинного обучения](https://en.wikipedia.org/wiki/Timeline_of_machine_learning)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za9A4I24xbYn"
   },
   "source": [
    "фреймворки для работы с нейросетями: \n",
    "- [TensorFlow](https://www.tensorflow.org/)\n",
    "- [Pytorch](https://pytorch.org/)\n",
    "- [Keras](https://keras.io/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP4hF8pKztdJ"
   },
   "source": [
    "<img src=\"nn_frameworks.png\" width=\"800\"/>\n",
    "\n",
    "[ссылка на полное сравнение](https://medium.com/analytics-vidhya/ml03-9de2f0dbd62d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qk8e3askGPnd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BFdbqvS5A4Fx",
    "PZkdQuB9A4F-",
    "V6aa1OeRwUz_",
    "T7XmcP8vwVXm",
    "3ufJZQjd6iES",
    "jhp5EQAPwVs3",
    "xHI2rv79A4GA",
    "CEi_rVWSx46r",
    "e6JI-eSLwzdX",
    "nf69AmXJoKyw",
    "fOhnsOkEw28X",
    "xZI2FfsTw3Xr",
    "9ZNgwU_Iw37H",
    "YxLO9Gu4t6VH"
   ],
   "name": "NN_intro_theory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}