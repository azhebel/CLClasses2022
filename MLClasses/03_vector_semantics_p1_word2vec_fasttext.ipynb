{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "vector semantics-1 (w2v, fasttext)",
   "provenance": [],
   "collapsed_sections": [
    "EH5xB1ecK8KB",
    "LA1eGZ4jT6vh",
    "p7YE0Pf8XLZ6",
    "doqTxplCDS2Q",
    "Ihkv6IG9ZfqC",
    "mYHaaRtdZRqn",
    "c8kt36LnooJa",
    "dqzH2MJbp-3t",
    "mvo7AB3h6nAL",
    "-mm1temk59fS",
    "KM7jcoLqyZut"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jR05b6WDS17"
   },
   "source": [
    "# Word2vec: работаем с векторными моделями в Python\n",
    "\n",
    "*Эта тетрадка — переосмысленная, дополненная и местами упрощенная мной версия туториала по вордтувеку от Лизы Кузьменко, которая со-основала RusVectores вместе с Андреем Кутузовым*\n",
    "\n",
    "\n",
    "**Word2vec** - библиотека для получения векторных представлений слов на основе их совместной встречаемости в текстах. Вы можете освежить в памяти механизмы работы **word2vec**, прочитав [эту статью](https://vk.com/@sysblok-word2vec-pokazhi-mne-svoi-kontekst-i-ya-skazhu-kto-ty). \n",
    "\n",
    "Сейчас мы научимся использовать **word2vec** в своей повседневной работе. Мы будем использовать реализацию **word2vec** в библиотеке [gensim](https://radimrehurek.com/gensim/) для языка программирования **python**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y4gMOGKqScy"
   },
   "source": [
    "Для работы с эмбеддингами слов существуют и другие библиотеки: кроме [gensim](https://radimrehurek.com/gensim/) можно делать векторные модели в [keras](https://keras.io/), [tensorflow](https://www.tensorflow.org/), [pytorch](https://pytorch.org/). Но мы будем работать с *gensim*, потому что тут это проще и потому что создатели библиотеки моделей RusVectores затачивались под нее.\n",
    "\n",
    "\n",
    "***Gensim***  - изначально библиотека для тематического моделирования текстов. Однако помимо различных алгоритмов для *topic modeling* в ней реализованы на **python** и алгоритмы из тулкита **word2vec** (который в оригинале был написан на C++). Если вы работаете на своей машине и **gensim** у вас не установлен, нужно его установить: `pip3 install gensim`\n",
    "\n",
    "В колабе генсим установлен по умолчанию\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TUCfUMi9KDwu"
   },
   "source": [
    "import gensim"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKROIDiIKEAj"
   },
   "source": [
    "Тьюториал состоит из двух частей:\n",
    "* В первой части мы разберёмся, как загружать уже готовые векторные модели и работать с ними. Например, мы научимся выполнять простые операции над векторами слов, такие как «найти слово с наиболее близким вектором» или «вычислить коэффициент близости между двумя векторами слов». Также мы рассмотрим более сложные операции над векторами, например, «вычесть из вектора слова вектор другого слова», «прибавить к вектору слова вектор другого слова»  «найти лишний вектор в группе слов».\n",
    "\n",
    "* Во второй части мы научимся предобрабатывать текстовые файлы и самостоятельно тренировать векторную модель на своих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCnjv9MbDS2N"
   },
   "source": [
    "## 1. Работа с готовыми векторными моделями при помощи библиотеки Gensim\n",
    "\n",
    "Для своих индивидуальных нужд и экспериментов бывает полезно самому натренировать модель на нужных данных и с нужными параметрами (об этом раздел 2). Но для каких-то общих целей уже есть готовые модели, в т.ч. для русского языка, обученные на больших корпусах\n",
    "\n",
    "Модели для русского скачать можно здесь - https://rusvectores.org/ru/models/\n",
    "\n",
    "Существуют несколько форматов, в которых могут храниться модели. Во-первых, данные могут храниться в нативном формате *word2vec*, при этом модель может быть бинарной или не бинарной. Для загрузки модели в формате *word2vec* в классе `KeyedVectors` (в котором хранится большинство относящихся к дистрибутивным моделям функций) существует функция `load_word2vec_format`, а бинарность модели можно указать в аргументе `binary` (внизу будет пример). Помимо этого, модель можно хранить и в собственном формате *gensim*, для этого существует класс `Word2Vec` с функцией `load`. Поскольку модели бывают разных форматов, то для них написаны разные функции загрузки; бывает полезно учитывать это в своем скрипте. Наш код определяет тип модели по её расширению, но вообще файл с моделью может называться как угодно, жестких ограничений для расширения нет.\n",
    "\n",
    "Давайте скачаем новейшую модель для русского языка, созданную на основе [Национального корпуса русского языка (НКРЯ)](http://www.ruscorpora.ru/) (поскольку zip-архив с моделью весит почти 500 мегабайт, следующая ячейка выполнится у вас не сразу!). \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQ4LBAljIr23",
    "outputId": "51504244-8439-4e98-812a-2a142a0ceb42"
   },
   "source": [
    "!wget 'http://vectors.nlpl.eu/repository/20/180.zip'"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-16 20:17:22--  http://vectors.nlpl.eu/repository/20/180.zip\r\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\r\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 484452317 (462M) [application/zip]\r\n",
      "Saving to: ‘180.zip’\r\n",
      "\r\n",
      "180.zip             100%[===================>] 462.01M  5.37MB/s    in 70s     \r\n",
      "\r\n",
      "2022-03-16 20:18:32 (6.59 MB/s) - ‘180.zip’ saved [484452317/484452317]\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90maopMCQAow"
   },
   "source": [
    "Теперь моделька в виде zip-архива лежит у нас в рабочей папке"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7uSZXD8NI9fX",
    "outputId": "95bf5bd7-e2c4-4282-b5de-7180fbec99a5"
   },
   "source": [
    "!ls"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.zip\r\n",
      "class1.ipynb\r\n",
      "spam.csv\r\n",
      "vector_semantics_1_(w2v,_fasttext).ipynb\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp6UO3tqJQhh"
   },
   "source": [
    "Распаковывать скачанный архив для обычных моделей не нужно, так как его содержимое прочитается при помощи специальной инструкции:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "79IZGnFGMWiz"
   },
   "source": [
    "import zipfile"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EvYOxVRADS2O"
   },
   "source": [
    "with zipfile.ZipFile('180.zip', 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "gensim.models.keyedvectors.KeyedVectors"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH5xB1ecK8KB"
   },
   "source": [
    "### Все, этой моделью уже можно пользоваться для оценки семантической близости:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk6fX0O1New_"
   },
   "source": [
    "Выводим 10 соседей слова по близости и меру близости с ними — метод `most_similar`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHn2dXOITmjI",
    "outputId": "eba80964-e937-46f9-810e-427662d076a4"
   },
   "source": [
    "model.most_similar ('вишня_NOUN')"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[('яблоня_NOUN', 0.6901825070381165),\n ('слива_NOUN', 0.6638981699943542),\n ('черешня_NOUN', 0.6636965870857239),\n ('абрикос_NOUN', 0.65395587682724),\n ('груша_NOUN', 0.6477444767951965),\n ('яблоко_NOUN', 0.6413966417312622),\n ('персик_NOUN', 0.6345474123954773),\n ('вишня_VERB', 0.63399738073349),\n ('алыча_NOUN', 0.6283605694770813),\n ('слива_ADJ', 0.612352728843689)]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTW4Tiv67Lig"
   },
   "source": [
    "⛳ 💻  Вопрос: как получить больше 10?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6KfUGn27uze",
    "outputId": "59f8181a-4878-4ad0-de3b-2e87ff01bb19"
   },
   "source": [
    "## ваш код\n",
    "model.most_similar ('лингвист_NOUN', topn=5)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[('языковед_NOUN', 0.7750651836395264),\n ('филолог_NOUN', 0.6346759796142578),\n ('лингвистика_NOUN', 0.629889190196991),\n ('антрополог_NOUN', 0.6117334365844727),\n ('лингвистический_ADJ', 0.607776939868927)]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFrok1K1NWQb",
    "outputId": "062db58c-da9f-4e2b-c4b4-40e6022332ef"
   },
   "source": [
    "model.most_similar ('программирование_NOUN')"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[('java_PROPN', 0.6837120652198792),\n ('алгоритмический_ADJ', 0.6621100306510925),\n ('интерфейс_NOUN', 0.6615059971809387),\n ('-технология_NOUN', 0.6228690147399902),\n ('xml_PROPN', 0.6216764450073242),\n ('html_PROPN', 0.6212592124938965),\n ('информатика_NOUN', 0.6158244013786316),\n ('пользовательский_ADJ', 0.6138737797737122),\n ('формализованный_ADJ', 0.6094743609428406),\n ('формализация_NOUN', 0.6024623513221741)]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "by3TyepsTo_2",
    "outputId": "b7c5fb17-fd15-4ceb-aa8a-67d6403b93d1"
   },
   "source": [
    "model.most_similar ('программировать_VERB')"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[('моделировать_VERB', 0.6391699910163879),\n ('запрограммированный_VERB', 0.6083677411079407),\n ('смоделировать_VERB', 0.5745291113853455),\n ('запрограммировать_VERB', 0.5709876418113708),\n ('просчитывать_VERB', 0.5686773657798767),\n ('задывать_VERB', 0.5433831810951233),\n ('детерминированный_VERB', 0.5256835222244263),\n ('предопределять_VERB', 0.5215480327606201),\n ('детерминировать_VERB', 0.5210214257240295),\n ('срабатывать_VERB', 0.509833574295044)]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA1eGZ4jT6vh"
   },
   "source": [
    "### 🤔 стоп, а что за _NOUN, _VERB это мне всегда руками так писать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyBvufWTSzPQ"
   },
   "source": [
    "Cлова в модель надо подавать с указанием части речи (pos tag) из набора тегов [Universal POS-tags](https://universaldependencies.org/u/pos/). Так устроены модели RusVectores — в них снята омонимия на уровне словоформ:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Um5xgucsSyVS",
    "outputId": "1493e707-5502-4a29-f183-a081dc73146e"
   },
   "source": [
    "model.most_similar ('печь_NOUN')"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[('печка_NOUN', 0.8553919792175293),\n ('печурка_NOUN', 0.6798753142356873),\n ('печной_ADJ', 0.6434558033943176),\n ('лежанка_NOUN', 0.6250245571136475),\n ('топка_NOUN', 0.6232640743255615),\n ('дымоход_NOUN', 0.6071035861968994),\n ('печи_NOUN', 0.602951169013977),\n ('чугун_NOUN', 0.5874652862548828),\n ('полать_NOUN', 0.5816735029220581),\n ('буржуйка_NOUN', 0.5790904760360718)]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Yra9RrYS7gn",
    "outputId": "a6296b3a-ebd9-4d91-a8ab-596c4577a38c"
   },
   "source": [
    "model.most_similar ('печь_VERB')"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[('испекать_VERB', 0.6701913475990295),\n ('выпекать_VERB', 0.6453807353973389),\n ('жарить_VERB', 0.622571587562561),\n ('жариться_VERB', 0.6005048155784607),\n ('напечь_VERB', 0.5875173211097717),\n ('стряпать_VERB', 0.5786464810371399),\n ('пек_NOUN', 0.5724241733551025),\n ('оладья_NOUN', 0.5647705793380737),\n ('запекать_VERB', 0.5645115971565247),\n ('варить_VERB', 0.561690092086792)]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH1mTtXeW9Mw"
   },
   "source": [
    "Давайте пока работаем с небольшим числом слов писать это руками. А дальше поговорим, как с этим работать автоматически, если анализируем большой текст. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kVL3_lSDS2P"
   },
   "source": [
    "Если мы прогоняем много слов, стоит вставить проверку, что слова нет в модели. Допустим, нам интересны такие слова (пример для русского языка):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nw-DIYJwDS2P"
   },
   "source": [
    "words = ['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'студент_NOUN', 'студент_ADJ']"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGgjvAo7DS2P"
   },
   "source": [
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ftf_MlfKDS2P",
    "outputId": "59a80c91-d345-4c67-de4a-77ec94d9259d"
   },
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print(word + ' is not present in the model')"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_NOUN\n",
      "неделя_NOUN 0.7375996112823486\n",
      "день_PROPN 0.706766664981842\n",
      "месяц_NOUN 0.7037326097488403\n",
      "час_NOUN 0.6643950939178467\n",
      "утро_NOUN 0.6526744961738586\n",
      "вечер_NOUN 0.6038411259651184\n",
      "сутки_NOUN 0.5923080444335938\n",
      "воскресенье_NOUN 0.5842781066894531\n",
      "полдень_NOUN 0.5743688344955444\n",
      "суббота_NOUN 0.5345946550369263\n",
      "\n",
      "\n",
      "ночь_NOUN\n",
      "ночь_PROPN 0.8310787081718445\n",
      "вечер_NOUN 0.7183678150177002\n",
      "рассвет_NOUN 0.6965947151184082\n",
      "ночи_NOUN 0.692021906375885\n",
      "полночь_NOUN 0.6704976558685303\n",
      "ночь_VERB 0.6615265011787415\n",
      "утро_NOUN 0.6263936161994934\n",
      "ночной_ADJ 0.6024709343910217\n",
      "полдень_NOUN 0.5835085511207581\n",
      "сумерки_NOUN 0.5671443939208984\n",
      "\n",
      "\n",
      "человек_NOUN\n",
      "человек_PROPN 0.7850059270858765\n",
      "человеческий_ADJ 0.5915265679359436\n",
      "существо_NOUN 0.5736929774284363\n",
      "народ_NOUN 0.5354466438293457\n",
      "личность_NOUN 0.5296981930732727\n",
      "человечество_NOUN 0.5282931327819824\n",
      "человкъ_PROPN 0.5047001242637634\n",
      "индивидуум_NOUN 0.5000404119491577\n",
      "нравственный_ADJ 0.4972919225692749\n",
      "потому_ADV 0.49293625354766846\n",
      "\n",
      "\n",
      "семантика_NOUN\n",
      "семантический_ADJ 0.8019332885742188\n",
      "синтаксический_ADJ 0.7569340467453003\n",
      "модальный_ADJ 0.7296056747436523\n",
      "семантически_ADV 0.7209396958351135\n",
      "смысловой_ADJ 0.7159028053283691\n",
      "референция_NOUN 0.7135109305381775\n",
      "ноэтический_ADJ 0.7080267071723938\n",
      "языковой_ADJ 0.7067197561264038\n",
      "лингвистический_ADJ 0.692865788936615\n",
      "предикат_NOUN 0.6877546906471252\n",
      "\n",
      "\n",
      "студент_NOUN\n",
      "преподаватель_NOUN 0.6743764281272888\n",
      "студенческий_ADJ 0.6486333012580872\n",
      "университетский_ADJ 0.6442698836326599\n",
      "заочник_NOUN 0.6423174142837524\n",
      "первокурсник_NOUN 0.640970766544342\n",
      "курсистка_NOUN 0.6364570260047913\n",
      "дипломник_NOUN 0.6341054439544678\n",
      "аспирант_NOUN 0.6337910890579224\n",
      "университет_NOUN 0.6302101016044617\n",
      "студентка_NOUN 0.6299037933349609\n",
      "\n",
      "\n",
      "студент_ADJ is not present in the model\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5FVSQtxU51J"
   },
   "source": [
    "Наш код сказал нам, что прилагательного студент модель не знает..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"сани_NOUN\" in model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7YE0Pf8XLZ6"
   },
   "source": [
    "### сравнить близость 2 слов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbGlUQJyDS2Q"
   },
   "source": [
    "Находим косинусную близость пары векторов слов — метод `similarity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6052179\n",
      "0.5960778\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('красный_ADJ', 'синий_ADJ'))\n",
    "print(model.similarity('красный_ADJ', 'зеленый_ADJ'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8A_SmuvDS2Q",
    "outputId": "fe5f35dc-f22e-42ef-c88c-b2c8bdeb1192"
   },
   "source": [
    "print(model.similarity('человек_NOUN', 'обезьяна_NOUN'))"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22025342\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3m5bK1E8LPOD",
    "outputId": "27daa7e1-8ee1-4fda-e1a5-3230646cb15f"
   },
   "source": [
    "print(model.similarity('человек_NOUN', 'женщина_NOUN'))\n",
    "print(model.similarity('человек_NOUN', 'мужчина_NOUN'))"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4803466\n",
      "0.36275825\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777514\n",
      "0.7041438\n",
      "0.7602008\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('мужчина_NOUN', 'женщина_NOUN'))\n",
    "print(model.similarity('король_NOUN', 'королева_NOUN'))\n",
    "print(model.similarity('принц_NOUN', 'принцесса_NOUN'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIUfy1H3LD04",
    "outputId": "7b9f4387-6eb8-42e1-ff20-09b2edaebbe3"
   },
   "source": [
    "print(model.similarity('человек_NOUN', 'картофель_NOUN'))"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030121876\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmNJWJYwQcXZ"
   },
   "source": [
    "<img src=\"https://www.meme-arsenal.com/memes/32687f97c291d55dc3143e26821ff4d4.jpg\">\n",
    "\n",
    "Мистер картошка, вы раскрыты! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doqTxplCDS2Q"
   },
   "source": [
    "## 2. Более сложные операции над векторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sXaupy_DS2R"
   },
   "source": [
    "Помимо более простых операций над векторами (нахождение косинусной близости между двумя векторами и ближайших соседей вектора) **gensim** позволяет выполнять и более сложные операции над несколькими векторами. Так, например, мы можем найти лишнее слово в группе. Лишним словом является то, вектор которого наиболее удален от других векторов слов."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3vQRNcaJN-z0"
   },
   "source": [
    "words = ['яблоко_NOUN', 'груша_NOUN', 'виноград_NOUN', 'банан_NOUN', 'лимон_NOUN', 'картофель_NOUN', 'лошадь_NOUN', 'философия_NOUN', 'видеоблог_NOUN']"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByxHRCybDS2R",
    "outputId": "fd5dc79b-ef2a-41e2-bcfd-0133270d055e"
   },
   "source": [
    "print(model.doesnt_match(words))"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "философия_NOUN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4vFdIVUDS2R"
   },
   "source": [
    "Также можно складывать и вычитать вектора нескольких слов. Например, сложив два вектора и вычтя из них третий вектор, мы можем решить своеобразную пропорцию. Подробнее о семантических пропорциях вы можете прочитать в [материале Системного Блока](https://vk.com/@sysblok-vo-chto-prevraschaetsya-zhizn-bez-lubvi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKymEqOIO912"
   },
   "source": [
    "Меня всегда радует, что вот это реально работает: \n",
    "🦅 -> 🐠"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dD0rpVnpDS2R",
    "outputId": "9a4821e5-094e-4e59-cf92-554c7ec0dd56"
   },
   "source": [
    "print(model.most_similar(positive=['рыба_NOUN', 'крыло_NOUN'], negative=['плавник_NOUN'])[0][0])"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "птица_NOUN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drYgxafWPY3N"
   },
   "source": [
    "В обратную сторону тоже: 🐠 -> 🦅 "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDeoPloNO4U-",
    "outputId": "ac360699-87c6-43a5-c5cc-f8b73a421a80"
   },
   "source": [
    "print(model.most_similar(positive=['птица_NOUN', 'плавник_NOUN'], negative=['крыло_NOUN'])[0][0])"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "рыба_NOUN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQkg4o54D4Lr"
   },
   "source": [
    "Ну и конечно: 💔"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLuIY3z3Xqjg",
    "outputId": "ca9866be-e640-43ee-ea38-cf7a146fa57d"
   },
   "source": [
    "print(model.most_similar(positive=['жизнь_NOUN'], negative=['любовь_NOUN'])[0][0])"
   ],
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "быт_NOUN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7mWkmOgEPEU"
   },
   "source": [
    "Ну и напоследок: шит хитз зе фэн 💩💩💩"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXZjlr_tD_1j",
    "outputId": "a521cc2e-6c76-4e04-fa59-ac28b40e066c"
   },
   "source": [
    "print(model.most_similar(positive=['ссср_PROPN', 'гитлер_PROPN'], negative=['германия_PROPN'])[0][0])"
   ],
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сталин_PROPN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('герцог_NOUN', 0.6892667412757874), ('король_PROPN', 0.6768986582756042), ('королева_NOUN', 0.6501403450965881), ('королева_ADV', 0.6257951855659485), ('дофин_NOUN', 0.6146250367164612)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['король_NOUN', 'женщина_NOUN'], negative=['мужчина_NOUN'], topn=5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "юрист_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['программист_NOUN'], negative=['клавиатура_NOUN'])[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('деньги_PROPN', 0.6096085906028748), ('сбережения_NOUN', 0.5187363028526306), ('денеод_NOUN', 0.5150483250617981), ('сумма_NOUN', 0.5042823553085327), ('средства_NOUN', 0.488772451877594)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['время_NOUN', 'деньги_NOUN'], topn=5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihkv6IG9ZfqC"
   },
   "source": [
    "### ⛳ 💻  Задание: подберите еще 3-4 симпатичных примера векторной арифметики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYHaaRtdZRqn"
   },
   "source": [
    "## 3. Предобработка текстовых данных\n",
    "\n",
    "Вернемся к вопросу о pos-тегах (_NOUN, _VERB и проч). Пока мы обходились без них, но это выглядело как костыль, правда же 🔩 Если мы хотим обрабатывать свои тексты — нам надо бы научиться предобрабатывать их так, чтобы каждое слово шло именно с таким тегом. Тогда можно будет гонять на них модели word2vec от RusVectores (а это лучшее что есть для русского и вообще такой стандартный стандарт).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHyLP1pJknEe"
   },
   "source": [
    "Предобработка текстов для тренировки моделей выглядит следующим образом:\n",
    "* сначала мы приведем все слова к начальной форме (лемматизируем) и удалим стоп-слова;\n",
    "* затем мы приведем все леммы к нижнему регистру;\n",
    "* для каждого слова добавим его частеречный тэг.\n",
    "\n",
    "Давайте попробуем воссоздать процесс предобработки текста на примере [сказки Хармса](https://raw.githubusercontent.com/dhhse/dh2020/master/data/harms.txt). Для предобработки можно использовать различные тэггеры, мы сейчас будем использовать [*UDPipe*](https://ufal.mff.cuni.cz/udpipe), чтобы сразу получить частеречную разметку в виде Universal POS-tags. Сначала установим обертку *UDPipe* для Python:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HmnqWoUkmZq",
    "outputId": "16cf7869-02da-48be-b164-e2e8a43f6b7f"
   },
   "source": [
    "!pip install ufal.udpipe"
   ],
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ufal.udpipe in /Users/Alexey.Zhebel/IdeaProjects/CompLing/venv/lib/python3.8/site-packages (1.2.0.3)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/Alexey.Zhebel/IdeaProjects/CompLing/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWpExSiWkqNA"
   },
   "source": [
    "*UDPipe* использует предобученные модели для лемматизации и тэггинга. Вы можете использовать [уже готовую модель](https://rusvectores.org/static/models/udpipe_syntagrus.model) или обучить свою. \n",
    "\n",
    "Кусок кода ниже скачает модель UDPipe для лингвистической предобработки. Модель весит 40 мегабайт, поэтому ячейка может выполнятся некоторое время, особенно если у вас небыстрый интернет. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUVSkATuDS2F",
    "outputId": "a490dd3c-5e11-44a7-8e2c-c2a8bc835eda"
   },
   "source": [
    "!wget 'https://rusvectores.org/static/models/udpipe_syntagrus.model'"
   ],
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-16 21:10:20--  https://rusvectores.org/static/models/udpipe_syntagrus.model\r\n",
      "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\r\n",
      "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 40616122 (39M)\r\n",
      "Saving to: ‘udpipe_syntagrus.model’\r\n",
      "\r\n",
      "udpipe_syntagrus.mo 100%[===================>]  38.73M  81.3KB/s    in 10m 12s \r\n",
      "\r\n",
      "2022-03-16 21:20:32 (64.8 KB/s) - ‘udpipe_syntagrus.model’ saved [40616122/40616122]\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7DptC4KhnnTF"
   },
   "source": [
    "modelfile = 'udpipe_syntagrus.model'"
   ],
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8kt36LnooJa"
   },
   "source": [
    "## ⚠️ Для соответствия моделям RusVectores требуется еще немножко допиливания напильником поверх UDPipe. Да и сама машинерия UDPipe довольно громоздко устроена (там выдача [в формате CONLLU](https://universaldependencies.org/format.html)). Поэтому ниже я просто переиспользую функции, которые написала со-авторка RusVectores Лиза Кузьменко для предобработки при помощи UDPipe и использую их. Но в целом там вроде бы ничего магического не происходит:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e8JL8zNDS2G"
   },
   "source": [
    "Приступим к собственно предобработке текста. Попробуем лемматизировать текст и добавить частеречные тэги при помощи этой функции:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qvGudRJeDS2G"
   },
   "source": [
    "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n"
   ],
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-StllA8WDS2H"
   },
   "source": [
    "Эту функцию можно также изменить под конкретную задачу. Например, если частеречные тэги нам не нужны, в функции ниже выставим `keep_pos=False`. Если необходимо сохранить знаки пунктуации, можно выставить `keep_punct=True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NK6OaUjkDS2H"
   },
   "source": [
    "Теперь загружаем модель *UDPipe*, читаем текстовый файл и обрабатываем его при помощи нашей функции. В файле должен содержаться необработанный текст (одно предложение на строку или один абзац на строку).\n",
    "Этот текст токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход мы получаем последовательность разделенных пробелами лемм с частями речи (\"зеленый\\_NOUN трамвай\\_NOUN\")."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYFeA8-nmkIY",
    "outputId": "5d412cbb-527e-4239-8645-3bf2c46fce6e"
   },
   "source": [
    "# в функции ниже используется питоновский модуль wget (не то же самое, что !wget выше)\n",
    "# на тот случай, если модель не скачана -- он ее автоматически перескачает\n",
    "# поэтому в этой ячейке установим питоновский wget\n",
    "!pip install wget"
   ],
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\r\n",
      "  Downloading wget-3.2.zip (10 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hBuilding wheels for collected packages: wget\r\n",
      "  Building wheel for wget (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9673 sha256=0210c80742a844c17a36e5760db5249181e9f8b3b049df48f1c2c25514f7bf09\r\n",
      "  Stored in directory: /Users/Alexey.Zhebel/Library/Caches/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\r\n",
      "Successfully built wget\r\n",
      "Installing collected packages: wget\r\n",
      "Successfully installed wget-3.2\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/Alexey.Zhebel/IdeaProjects/CompLing/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9_G3ibIcDS2I"
   },
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import wget\n",
    "\n",
    "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
    "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "    if not os.path.isfile(modelfile):\n",
    "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "        wget.download(udpipe_model_url)\n",
    "\n",
    "    print('\\nLoading the model...', file=sys.stderr)\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    print('Processing input...', file=sys.stderr)\n",
    "    lines = text.split('\\n')\n",
    "    tagged = []\n",
    "    for line in lines:\n",
    "        # line = unify_sym(line.strip()) # здесь могла бы быть ваша функция очистки текста\n",
    "        output = process(process_pipeline, text=line)\n",
    "        tagged_line = ' '.join(output)\n",
    "        tagged.append(tagged_line)\n",
    "    return '\\n'.join(tagged)\n",
    "\n",
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken"
   ],
   "execution_count": 74,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VcD_86bAl0t"
   },
   "source": [
    "Скачаем теперь текст, с которым будем работать:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDqsRQ5JmFad",
    "outputId": "a19210f1-57eb-4828-ee04-b3b84b29a19c"
   },
   "source": [
    "!wget 'https://raw.githubusercontent.com/dhhse/dh2020/master/data/harms.txt'"
   ],
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-16 21:42:02--  https://raw.githubusercontent.com/dhhse/dh2020/master/data/harms.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 8147 (8.0K) [text/plain]\r\n",
      "Saving to: ‘harms.txt’\r\n",
      "\r\n",
      "harms.txt           100%[===================>]   7.96K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2022-03-16 21:42:02 (28.8 MB/s) - ‘harms.txt’ saved [8147/8147]\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3Ys8_xQDS2J",
    "outputId": "b251f7e2-607b-4fc0-d44a-0fa5d0178a69"
   },
   "source": [
    "skazka = open('harms.txt', 'r', encoding='utf-8').read()\n",
    "processed_text = tag_ud(text=skazka, modelfile=modelfile)\n",
    "print(processed_text[:350])\n",
    "with open('harms_processed.txt', 'w', encoding='utf-8') as out:\n",
    "    out.write(processed_text)"
   ],
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сказка_NOUN\n",
      "вот_PART сказать_VERB Ваня_PROPN  класть_VERB на_ADP стол_NOUN тетрадка_NOUN давать_VERB писать_VERB сказка_NOUN\n",
      "давать_VERB сказать_VERB Леночка_PROPN  садиться_VERB на_ADP стул_NOUN\n",
      "Ваня_PROPN  брать_VERB карандаш_NOUN и_CCONJ писать_VERB\n",
      "Жил-был_VERB король_NOUN\n",
      "тут_ADV Ваня_PROPN  задумываться_VERB и_CCONJ поднимать_VERB глаз_NOUN к\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L__c7FzUDS2K"
   },
   "source": [
    "Наша функция напечатает обработанный текст, который мы теперь можем также сохранить в файл. \n",
    "\n",
    "Итак, в ходе этой части тьюториала мы научились от \"сырого текста\" приходить к лемматизированному тексту с частеречными тэгами, который уже можно подавать на вход модели! \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erNk9KEo9R2E",
    "outputId": "f45f62e4-9c82-4743-fdd2-ce05d95f8b40"
   },
   "source": [
    "with open('harms_processed.txt', 'r', encoding='utf-8') as tagged_text:\n",
    "    words = tagged_text.read().split()\n",
    "    for word in words[:20]:\n",
    "      print ('слово: ', word)\n",
    "      if word in model:\n",
    "        x = model.most_similar(word)\n",
    "        print ('ближайший синоним: ', x[0][0])"
   ],
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "слово:  Сказка_NOUN\n",
      "слово:  вот_PART\n",
      "слово:  сказать_VERB\n",
      "ближайший синоним:  говорить_VERB\n",
      "слово:  Ваня_PROPN\n",
      "слово:  класть_VERB\n",
      "ближайший синоним:  положить_VERB\n",
      "слово:  на_ADP\n",
      "слово:  стол_NOUN\n",
      "ближайший синоним:  столик_NOUN\n",
      "слово:  тетрадка_NOUN\n",
      "ближайший синоним:  тетрадь_NOUN\n",
      "слово:  давать_VERB\n",
      "ближайший синоним:  дать_VERB\n",
      "слово:  писать_VERB\n",
      "ближайший синоним:  написать_VERB\n",
      "слово:  сказка_NOUN\n",
      "ближайший синоним:  сказка_PROPN\n",
      "слово:  давать_VERB\n",
      "ближайший синоним:  дать_VERB\n",
      "слово:  сказать_VERB\n",
      "ближайший синоним:  говорить_VERB\n",
      "слово:  Леночка_PROPN\n",
      "слово:  садиться_VERB\n",
      "ближайший синоним:  усаживаться_VERB\n",
      "слово:  на_ADP\n",
      "слово:  стул_NOUN\n",
      "ближайший синоним:  табурет_NOUN\n",
      "слово:  Ваня_PROPN\n",
      "слово:  брать_VERB\n",
      "ближайший синоним:  взять_VERB\n",
      "слово:  карандаш_NOUN\n",
      "ближайший синоним:  карандашик_NOUN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kjx6ZCuxAzFG"
   },
   "source": [
    "[Вернемся к слайдам ненадолго](https://docs.google.com/presentation/d/11fYkNG1IFBJzVQ27LNxaE_fj8XHO40JxYaUUSzpcCKQ/edit#slide=id.gd5da728dca_0_1009) — нам осталась fun part сегодняшней пары! 🎪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTkDErYXA2t1"
   },
   "source": [
    "⛳ 💻  Я предлагаю вам реализовать свою версию  [векторных романов Б. Орехова](https://habr.com/ru/post/326380/). В базовом варианте предлагаю делать упрощенно: без восстановления морфологической формы после замены слова на его векторный синоним (можем для корректности назвать это семантическим ассоциатом или квази-синонимом). Ну то есть заменяем \"бегемотом\" на \"гиппопотам\" (или что там выдаст word2vec), но форму \"гиппопотамом\" уже не восстанавливаем. \n",
    "\n",
    "А кто чувствует в себе силы — делайте полную версию, с восстановлением исходной грамматической формы \"гиппопотамом\". За это оценка будет выше. Ну и результат у вас будет гораздо прикольнее, потому что текст будет довольно читабелен.\n",
    "\n",
    "К <b><s>10</s> 16 мая</b> нужно сдать любое крупное произведение русской литературы, в котором слова заменены на их векторные семантические ассоциаты. Ну и, конечно, код, который это делает. Код из этой тетрадки можно переиспользовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqzH2MJbp-3t"
   },
   "source": [
    "## 4. Тренируем свою модель в gensim (остается на домашние эксперименты и следующий раз)\n",
    "\n",
    "Поскольку обучение и загрузка моделей могут занимать продолжительное время, иногда бывает полезно вести лог событий. Для этого используется стандартная питоновская библиотека **logging**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e6P23Mg6DS2L"
   },
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ],
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRdlTAkXDS2L"
   },
   "source": [
    "На вход модели мы даем наш обработанный текстовый файл (либо любой другой текст, важно лишь, что каждое предложение должно быть на отдельной строчке)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0aQoK8ydDS2L"
   },
   "source": [
    "f = 'harms_processed.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ],
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkxT-JJqDS2L"
   },
   "source": [
    "Инициализируем модель. Параметры в скобочках:\n",
    "* data - данные, \n",
    "* size - размер вектора, \n",
    "* window - размер окна наблюдения,\n",
    "* min_count - мин. частотность слова в корпусе, которое мы берем,\n",
    "* sg - используемый алгоритм обучение (0 - CBOW, 1 - Skip-gram))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDPJ3kEPjQLZ",
    "outputId": "0f6fd594-a27e-4595-c150-feddfc590f95"
   },
   "source": [
    "model = gensim.models.Word2Vec(data, vector_size=500, window=10, min_count=2, sg=0)"
   ],
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 22:07:09,980 : INFO : collecting all words and their counts\n",
      "2022-03-16 22:07:09,981 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-03-16 22:07:09,982 : INFO : collected 270 word types from a corpus of 690 raw words and 39 sentences\n",
      "2022-03-16 22:07:09,983 : INFO : Creating a fresh vocabulary\n",
      "2022-03-16 22:07:09,984 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 111 unique words (41.111111111111114%% of original 270, drops 159)', 'datetime': '2022-03-16T22:07:09.984207', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-03-16 22:07:09,984 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 531 word corpus (76.95652173913044%% of original 690, drops 159)', 'datetime': '2022-03-16T22:07:09.984866', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-03-16 22:07:09,986 : INFO : deleting the raw counts dictionary of 270 items\n",
      "2022-03-16 22:07:09,987 : INFO : sample=0.001 downsamples 111 most-common words\n",
      "2022-03-16 22:07:09,987 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 221.68669714887892 word corpus (41.7%% of prior 531)', 'datetime': '2022-03-16T22:07:09.987602', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-03-16 22:07:09,990 : INFO : estimated required memory for 111 words and 500 dimensions: 499500 bytes\n",
      "2022-03-16 22:07:09,990 : INFO : resetting layer weights\n",
      "2022-03-16 22:07:09,992 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-03-16T22:07:09.992540', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2022-03-16 22:07:09,993 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 111 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-03-16T22:07:09.993163', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-03-16 22:07:09,997 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-16 22:07:09,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-16 22:07:09,998 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-16 22:07:09,998 : INFO : EPOCH - 1 : training on 690 raw words (223 effective words) took 0.0s, 67771 effective words/s\n",
      "2022-03-16 22:07:10,001 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-16 22:07:10,001 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-16 22:07:10,001 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-16 22:07:10,002 : INFO : EPOCH - 2 : training on 690 raw words (223 effective words) took 0.0s, 103046 effective words/s\n",
      "2022-03-16 22:07:10,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-16 22:07:10,004 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-16 22:07:10,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-16 22:07:10,005 : INFO : EPOCH - 3 : training on 690 raw words (203 effective words) took 0.0s, 92459 effective words/s\n",
      "2022-03-16 22:07:10,008 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-16 22:07:10,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-16 22:07:10,009 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-16 22:07:10,010 : INFO : EPOCH - 4 : training on 690 raw words (209 effective words) took 0.0s, 68905 effective words/s\n",
      "2022-03-16 22:07:10,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-16 22:07:10,013 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-16 22:07:10,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-16 22:07:10,013 : INFO : EPOCH - 5 : training on 690 raw words (225 effective words) took 0.0s, 104108 effective words/s\n",
      "2022-03-16 22:07:10,014 : INFO : Word2Vec lifecycle event {'msg': 'training on 3450 raw words (1083 effective words) took 0.0s, 51991 effective words/s', 'datetime': '2022-03-16T22:07:10.014452', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-03-16 22:07:10,015 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=111, vector_size=500, alpha=0.025)', 'datetime': '2022-03-16T22:07:10.014998', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4wU8u-WbDS2M"
   },
   "source": [
    "# вариант для новой версии gensim:\n",
    "# model = gensim.models.Word2Vec(data, vector_size=500, window=10, min_count=2, sg=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNWoIa4wDS2M"
   },
   "source": [
    "Мы создаем модель, в которой размерность векторов — 500, размер окна наблюдения — 10 слов, алгоритм обучения — CBOW, слова, встретившиеся в корпусе только 1 раз, не используются. После тренировки модели можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировать."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImrKLGf9DS2M",
    "outputId": "169ff1ef-1518-4d5d-f59c-f086a8011f24"
   },
   "source": [
    "model.init_sims(replace=True)"
   ],
   "execution_count": 84,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/jlv0gzhn72vdj8t4yttnz9cw0000kt/T/ipykernel_15631/3625671219.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2022-03-16 22:20:41,591 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_bxgIYeDS2M"
   },
   "source": [
    "Смотрим, сколько в модели слов:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YOCjNmWDS2N",
    "outputId": "a44f204c-fb24-48d8-d080-c8a576ba0d0a"
   },
   "source": [
    "print(len(model.wv.vocab))"
   ],
   "execution_count": 89,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/b6/jlv0gzhn72vdj8t4yttnz9cw0000kt/T/ipykernel_15631/3915736557.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/IdeaProjects/CompLing/venv/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mvocab\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    659\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    660\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mvocab\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 661\u001B[0;31m         raise AttributeError(\n\u001B[0m\u001B[1;32m    662\u001B[0m             \u001B[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    663\u001B[0m             \u001B[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.key_to_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km8YogT4DS2N"
   },
   "source": [
    "И сохраняем!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrfSGHKhDS2N",
    "outputId": "c1c2388c-eb4c-4e7f-a4a3-724fe0ec686f"
   },
   "source": [
    "model.save('my.model')"
   ],
   "execution_count": 91,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 22:23:17,042 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'my.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-03-16T22:23:17.042630', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-03-16 22:23:17,043 : INFO : not storing attribute cum_table\n",
      "2022-03-16 22:23:17,045 : INFO : saved my.model\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RS5ben4LwMth",
    "outputId": "64b59fc2-203c-4bf1-a261-be799bbf0665"
   },
   "source": [
    "model.most_similar('нога_NOUN')"
   ],
   "execution_count": 95,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'most_similar'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/b6/jlv0gzhn72vdj8t4yttnz9cw0000kt/T/ipykernel_15631/2762854963.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmost_similar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'нога_NOUN'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: 'Word2Vec' object has no attribute 'most_similar'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbVMzxuyl9TC",
    "outputId": "e9131567-07c2-4740-d4bb-a7038f8bec84"
   },
   "source": [
    "# для новой версии генсим, где не работает просто most_similar:\n",
    "model.wv.most_similar ('карандаш_NOUN')"
   ],
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "[('и_CCONJ', 0.14096979796886444),\n ('комната_NOUN', 0.11021428555250168),\n ('1-е_NOUN', 0.10871057212352753),\n ('о_ADP', 0.10352557897567749),\n ('писать_VERB', 0.1028280258178711),\n ('падать_VERB', 0.09768801927566528),\n ('тетрадка_NOUN', 0.09694117307662964),\n ('кузнеце_NOUN', 0.09089557826519012),\n ('кулак_NOUN', 0.08956850320100784),\n ('там_ADV', 0.08054307848215103)]"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvo7AB3h6nAL"
   },
   "source": [
    "### ⛳ 💻 Задание\n",
    "\n",
    "Задание: попробуйте обучить свою модель на каких-нибудь текстах. Например, [вот текст \"Войны и мира\"](https://github.com/dhhse/dh2020/blob/master/data/wap_w2v.txt), в котором каждое предложение с новой строки. Обучите модель на нем. Исследуйте близости слов. \n",
    "\n",
    "*   Подсказка: вам точно понадобится `gensim.models.word2vec.LineSentence` (см. выше) чтобы преобразовать текст в формат, который можно передавать для обучения модели\n",
    "*   Подсказка 2: а еще вам понадобится `gensim.models.Word2Vec` (см. выше)\n",
    "*   Подсказка 3: без остального в принципе можно обойтись. Но если подать тексты сырыми, то модель обучится на токенах. \"Кот\", \"кот\" и \"кота\" будут для нее разными словами. Зато \"печь\" будет одним словом вне завимисимости от части речи. Поэтому можно еще воспользоваться функцией `tag_ud` выше и обучить модель как у русвекторес. Но осторожно. UDPipe-ом обработать всю Войну и Мир — это минут 6-7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYX3PTisK143"
   },
   "source": [
    "Без предобработки ваша модель сможет как-то вот так: \n",
    "\n",
    "<img src = \"https://github.com/dhhse/dh2020/raw/master/pics/napoleon_token.png\">\n",
    "<img src = \"https://github.com/dhhse/dh2020/raw/master/pics/war_token.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjLGKy6JMPZG"
   },
   "source": [
    "C предобработкой будет как в моделях русвекторес: лемма с пос-тегом. И это на русском работает осмысленнее обычно: \n",
    "\n",
    "<img src = \"https://github.com/dhhse/dh2020/raw/master/pics/war_lemma_pos.png\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueUidTPFbkqI",
    "outputId": "adcf00ad-6f99-48c6-fea1-5069230bbb21"
   },
   "source": [
    "!wget 'https://github.com/dhhse/dh2020/raw/master/data/wap_w2v.txt' "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2021-05-13 15:53:10--  https://github.com/dhhse/dh2020/raw/master/data/wap_w2v.txt\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/dhhse/dh2020/master/data/wap_w2v.txt [following]\n",
      "--2021-05-13 15:53:10--  https://raw.githubusercontent.com/dhhse/dh2020/master/data/wap_w2v.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5171830 (4.9M) [text/plain]\n",
      "Saving to: ‘wap_w2v.txt’\n",
      "\n",
      "wap_w2v.txt         100%[===================>]   4.93M  25.6MB/s    in 0.2s    \n",
      "\n",
      "2021-05-13 15:53:11 (25.6 MB/s) - ‘wap_w2v.txt’ saved [5171830/5171830]\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wK9p9ez4br0m"
   },
   "source": [
    "wap = 'wap_w2v.txt'\n",
    "data = gensim.models.word2vec.LineSentence(wap)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_TLGKKVb92E",
    "outputId": "486fa883-236c-4035-ce6e-d7990cac9649"
   },
   "source": [
    "model_wap = gensim.models.Word2Vec(data, size=500, window=10, min_count=2, sg=0)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-05-13 15:54:45,536 : INFO : collecting all words and their counts\n",
      "2021-05-13 15:54:45,539 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-05-13 15:54:45,628 : INFO : PROGRESS: at sentence #10000, processed 161603 words, keeping 39106 word types\n",
      "2021-05-13 15:54:45,717 : INFO : PROGRESS: at sentence #20000, processed 335781 words, keeping 65326 word types\n",
      "2021-05-13 15:54:45,786 : INFO : collected 81592 word types from a corpus of 462159 raw words and 27123 sentences\n",
      "2021-05-13 15:54:45,789 : INFO : Loading a fresh vocabulary\n",
      "2021-05-13 15:54:45,864 : INFO : effective_min_count=2 retains 30236 unique words (37% of original 81592, drops 51356)\n",
      "2021-05-13 15:54:45,866 : INFO : effective_min_count=2 leaves 410803 word corpus (88% of original 462159, drops 51356)\n",
      "2021-05-13 15:54:45,959 : INFO : deleting the raw counts dictionary of 81592 items\n",
      "2021-05-13 15:54:45,964 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2021-05-13 15:54:45,966 : INFO : downsampling leaves estimated 342650 word corpus (83.4% of prior 410803)\n",
      "2021-05-13 15:54:46,043 : INFO : estimated required memory for 30236 words and 500 dimensions: 136062000 bytes\n",
      "2021-05-13 15:54:46,044 : INFO : resetting layer weights\n",
      "2021-05-13 15:54:52,191 : INFO : training model with 3 workers on 30236 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-05-13 15:54:53,245 : INFO : EPOCH 1 - PROGRESS: at 66.05% examples, 211092 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-13 15:54:53,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 15:54:53,688 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 15:54:53,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 15:54:53,707 : INFO : EPOCH - 1 : training on 462159 raw words (342450 effective words) took 1.5s, 226671 effective words/s\n",
      "2021-05-13 15:54:54,802 : INFO : EPOCH 2 - PROGRESS: at 67.85% examples, 211605 words/s, in_qsize 5, out_qsize 1\n",
      "2021-05-13 15:54:55,172 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 15:54:55,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 15:54:55,221 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 15:54:55,222 : INFO : EPOCH - 2 : training on 462159 raw words (342366 effective words) took 1.5s, 226933 effective words/s\n",
      "2021-05-13 15:54:56,248 : INFO : EPOCH 3 - PROGRESS: at 57.21% examples, 188463 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-13 15:54:56,830 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 15:54:56,859 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 15:54:56,869 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 15:54:56,870 : INFO : EPOCH - 3 : training on 462159 raw words (342493 effective words) took 1.6s, 208929 effective words/s\n",
      "2021-05-13 15:54:57,884 : INFO : EPOCH 4 - PROGRESS: at 66.05% examples, 219679 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-13 15:54:58,354 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 15:54:58,359 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 15:54:58,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 15:54:58,372 : INFO : EPOCH - 4 : training on 462159 raw words (342766 effective words) took 1.5s, 229066 effective words/s\n",
      "2021-05-13 15:54:59,413 : INFO : EPOCH 5 - PROGRESS: at 63.81% examples, 207187 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-13 15:54:59,874 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 15:54:59,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 15:54:59,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 15:54:59,910 : INFO : EPOCH - 5 : training on 462159 raw words (342580 effective words) took 1.5s, 223881 effective words/s\n",
      "2021-05-13 15:54:59,914 : INFO : training on a 2310795 raw words (1712655 effective words) took 7.7s, 221801 effective words/s\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7siNtzVcDNR",
    "outputId": "3994f30f-6701-44b2-b1b1-fb36751aadec"
   },
   "source": [
    "model_wap.most_similar('Наташа')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('долго', 0.9984106421470642),\n",
       " ('Соню,', 0.9982855319976807),\n",
       " ('что-то', 0.9982360601425171),\n",
       " ('нее,', 0.9981175065040588),\n",
       " ('за', 0.9978007078170776),\n",
       " ('Наташе,', 0.9977572560310364),\n",
       " ('грудь', 0.997657299041748),\n",
       " ('заметила,', 0.9976304769515991),\n",
       " ('слушал', 0.9974093437194824),\n",
       " ('где', 0.9973325729370117)]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 60
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "np3N5_1EgpiM",
    "outputId": "376e94f8-9717-4738-98f5-6e5c1bfdb558"
   },
   "source": [
    "len(model_wap.wv.vocab)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30236"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 61
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rT2ehmkHcbhq",
    "outputId": "023d175e-a964-4936-ce5c-e0a533dd5eb1"
   },
   "source": [
    "text = open('wap_w2v.txt', 'r', encoding='utf-8').read()\n",
    "processed_text = tag_ud(text=text, modelfile=modelfile)\n",
    "print(processed_text[:350])\n",
    "with open('wap_w2v_processed.txt', 'w', encoding='utf-8') as out:\n",
    "    out.write(processed_text)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "то_PRON первый_ADJ\n",
      "\n",
      "часть_NOUN первый_ADJ\n",
      "\n",
      "i_NUM\n",
      "\n",
      "Eh_PROPN bien_X mon_X prince_X\n",
      "Gênes_PROPN et_X Lucques_PROPN ne_X sont_X plus_X que_X des_X apanages_X des_X поместье_NOUN de_X la_X famille_X Buonaparte_PROPN\n",
      "Non_PROPN je_X vous_X préviens_X que_X si_X vous_X ne_X me_X dites_X pas_X que_X nous_X avons_X la_X guerre_X si_X vous_X vous_X permettez_\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i3f-1IrFctHA"
   },
   "source": [
    "data = gensim.models.word2vec.LineSentence('wap_w2v_processed.txt')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qr_AKEk-c0kb",
    "outputId": "e69f4bef-3dac-402c-d1a7-1a8725545b94"
   },
   "source": [
    "model_wap_lemmas_pos = gensim.models.Word2Vec(data, size=300, window=10, min_count=2, sg=0)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-05-13 16:07:24,984 : INFO : collecting all words and their counts\n",
      "2021-05-13 16:07:24,987 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-05-13 16:07:25,076 : INFO : PROGRESS: at sentence #10000, processed 156252 words, keeping 15798 word types\n",
      "2021-05-13 16:07:25,181 : INFO : PROGRESS: at sentence #20000, processed 324927 words, keeping 24155 word types\n",
      "2021-05-13 16:07:25,263 : INFO : collected 29037 word types from a corpus of 447977 raw words and 27064 sentences\n",
      "2021-05-13 16:07:25,269 : INFO : Loading a fresh vocabulary\n",
      "2021-05-13 16:07:25,300 : INFO : effective_min_count=2 retains 13506 unique words (46% of original 29037, drops 15531)\n",
      "2021-05-13 16:07:25,301 : INFO : effective_min_count=2 leaves 432446 word corpus (96% of original 447977, drops 15531)\n",
      "2021-05-13 16:07:25,343 : INFO : deleting the raw counts dictionary of 29037 items\n",
      "2021-05-13 16:07:25,346 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2021-05-13 16:07:25,348 : INFO : downsampling leaves estimated 341275 word corpus (78.9% of prior 432446)\n",
      "2021-05-13 16:07:25,381 : INFO : estimated required memory for 13506 words and 300 dimensions: 39167400 bytes\n",
      "2021-05-13 16:07:25,382 : INFO : resetting layer weights\n",
      "2021-05-13 16:07:28,074 : INFO : training model with 3 workers on 13506 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-05-13 16:07:29,084 : INFO : EPOCH 1 - PROGRESS: at 90.44% examples, 302352 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-13 16:07:29,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:07:29,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:07:29,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:07:29,158 : INFO : EPOCH - 1 : training on 447977 raw words (341385 effective words) took 1.1s, 316128 effective words/s\n",
      "2021-05-13 16:07:30,173 : INFO : EPOCH 2 - PROGRESS: at 92.46% examples, 309685 words/s, in_qsize 4, out_qsize 0\n",
      "2021-05-13 16:07:30,197 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:07:30,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:07:30,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:07:30,230 : INFO : EPOCH - 2 : training on 447977 raw words (341314 effective words) took 1.1s, 320855 effective words/s\n",
      "2021-05-13 16:07:31,245 : INFO : EPOCH 3 - PROGRESS: at 92.47% examples, 309533 words/s, in_qsize 4, out_qsize 0\n",
      "2021-05-13 16:07:31,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:07:31,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:07:31,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:07:31,317 : INFO : EPOCH - 3 : training on 447977 raw words (341496 effective words) took 1.1s, 316306 effective words/s\n",
      "2021-05-13 16:07:32,327 : INFO : EPOCH 4 - PROGRESS: at 83.56% examples, 280491 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-13 16:07:32,449 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:07:32,455 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:07:32,477 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:07:32,478 : INFO : EPOCH - 4 : training on 447977 raw words (341282 effective words) took 1.2s, 295714 effective words/s\n",
      "2021-05-13 16:07:33,499 : INFO : EPOCH 5 - PROGRESS: at 94.72% examples, 314847 words/s, in_qsize 3, out_qsize 0\n",
      "2021-05-13 16:07:33,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:07:33,535 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:07:33,540 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:07:33,541 : INFO : EPOCH - 5 : training on 447977 raw words (341338 effective words) took 1.1s, 323064 effective words/s\n",
      "2021-05-13 16:07:33,543 : INFO : training on a 2239885 raw words (1706815 effective words) took 5.5s, 312117 effective words/s\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXvtzGvbc2Gu",
    "outputId": "eff19262-e9d1-4d33-ed4d-cc4bbbc48154"
   },
   "source": [
    "model_wap_lemmas_pos.most_similar ('Наташа_PROPN')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Пьер_PROPN', 0.9753262996673584),\n",
       " ('Марья_PROPN', 0.9669135808944702),\n",
       " ('осведомляться_VERB', 0.9598015546798706),\n",
       " ('удивление_NOUN', 0.9585548639297485),\n",
       " ('письмо_NOUN', 0.9561311602592468),\n",
       " ('отъезд_NOUN', 0.9557530283927917),\n",
       " ('знакомый_NOUN', 0.9538062810897827),\n",
       " ('холодность_NOUN', 0.9526064395904541),\n",
       " ('разговор_NOUN', 0.9524383544921875),\n",
       " ('смутный_ADJ', 0.9505127668380737)]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 69
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bm2exnhMfhAQ",
    "outputId": "c91145d0-e6d1-437a-be51-ef5e826266f4"
   },
   "source": [
    "len(model_wap_lemmas_pos.wv.vocab)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13506"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 57
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KouFnFeLM7a"
   },
   "source": [
    "\n",
    "## FastText: эмбеддинги n-граммов\n",
    "\n",
    "FastText использует не только эмбеддинги слов, но и эмбеддинги n-грамов. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2ssCoFXUeOH"
   },
   "source": [
    "### Для работы с fasttext-моделью придется обновить gensim\n",
    "\n",
    "Корректная работа с fasttext-моделями гарантируется от версии 3.7.2, а в колабе по умолчанию генсим 3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e5eXNJ1qqOeh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "9fe8ba48-3414-4f42-ad89-4bbbc96a2de1"
   },
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ],
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "'4.1.2'"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bf3zrCMDRdlQ"
   },
   "source": [
    "!pip install --upgrade gensim"
   ],
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/Alexey.Zhebel/IdeaProjects/CompLing/venv/lib/python3.8/site-packages (4.1.2)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/Alexey.Zhebel/IdeaProjects/CompLing/venv/lib/python3.8/site-packages (from gensim) (1.7.3)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/Alexey.Zhebel/IdeaProjects/CompLing/venv/lib/python3.8/site-packages (from gensim) (5.2.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/Alexey.Zhebel/IdeaProjects/CompLing/venv/lib/python3.8/site-packages (from gensim) (1.21.5)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/Alexey.Zhebel/IdeaProjects/CompLing/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgmcSYO0UsY9"
   },
   "source": [
    "После этого надо перезапустить среду (колаб и сам вам предложит это сделать)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9SBS53pRx9i",
    "outputId": "2ce574a6-9f1f-483d-c272-0603ff9c51d2"
   },
   "source": [
    "import gensim"
   ],
   "execution_count": 99,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RH5lqIVqRS7z",
    "outputId": "a39c6e4b-a5db-4336-e639-ebe6c565bd70"
   },
   "source": [
    "gensim.__version__"
   ],
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "'4.1.2'"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zsJhprjVKxB"
   },
   "source": [
    "### Отлично, теперь возьмем русскую фасттекст-модель \n",
    "из уже известной нам [коллекции](https://rusvectores.org/ru/models/) RusVectores"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ubhi2tebLLuc",
    "outputId": "6c32140b-7525-4956-8720-0fc9fd17f61f"
   },
   "source": [
    "!wget 'http://vectors.nlpl.eu/repository/20/214.zip'"
   ],
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-19 11:25:58--  http://vectors.nlpl.eu/repository/20/214.zip\r\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\r\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1920218982 (1.8G) [application/zip]\r\n",
      "Saving to: ‘214.zip’\r\n",
      "\r\n",
      "214.zip             100%[===================>]   1.79G  7.25MB/s    in 4m 29s  \r\n",
      "\r\n",
      "2022-03-19 11:30:28 (6.81 MB/s) - ‘214.zip’ saved [1920218982/1920218982]\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeHEbke4pY3Y"
   },
   "source": [
    "В генсим её надо загружать чуть иначе, чем word2vec-овскую модель. Надо сначала распаковать архив:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Fb2rZl8TJmD",
    "outputId": "e284a4be-59e7-4a75-95cf-6e30fb048f32"
   },
   "source": [
    "!unzip '214.zip'"
   ],
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  214.zip\r\n",
      "  inflating: meta.json               \r\n",
      "  inflating: model.model             \r\n",
      "  inflating: model.model.vectors_ngrams.npy  \r\n",
      "  inflating: model.model.vectors.npy  \r\n",
      "  inflating: model.model.vectors_vocab.npy  \r\n",
      "  inflating: README                  \r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rf0wT0t5TWer"
   },
   "source": [
    "fasttext_model = gensim.models.KeyedVectors.load('model.model')"
   ],
   "execution_count": 103,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-19 11:30:49,920 : INFO : loading KeyedVectors object from model.model\n",
      "2022-03-19 11:30:51,327 : INFO : loading vectors from model.model.vectors.npy with mmap=None\n",
      "2022-03-19 11:30:51,559 : INFO : loading vectors_vocab from model.model.vectors_vocab.npy with mmap=None\n",
      "2022-03-19 11:30:51,768 : INFO : loading vectors_ngrams from model.model.vectors_ngrams.npy with mmap=None\n",
      "2022-03-19 11:30:52,395 : INFO : setting ignored attribute vectors_norm to None\n",
      "2022-03-19 11:30:52,396 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2022-03-19 11:30:52,396 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2022-03-19 11:30:52,397 : INFO : setting ignored attribute buckets_word to None\n",
      "2022-03-19 11:30:58,580 : INFO : FastTextKeyedVectors lifecycle event {'fname': 'model.model', 'datetime': '2022-03-19T11:30:58.580209', 'gensim': '4.1.2', 'python': '3.8.9 (default, Oct 26 2021, 07:25:54) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.1-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mm1temk59fS"
   },
   "source": [
    "### косинусная близость на примерах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "[('экспрессом', 0.82410728931427),\n ('экспрессов', 0.8085681796073914),\n ('экспресса', 0.7968765497207642),\n ('экспрессы', 0.7598093748092651),\n ('экспрессе', 0.7217636108398438),\n ('экспрессии', 0.6977290511131287),\n ('экспрессия', 0.688822329044342),\n ('экспресс', 0.6845555901527405),\n ('экспрессию', 0.6839092373847961),\n ('экспрессией', 0.6713719964027405)]"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.most_similar(\"экспрессо\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fM7-Bw4KXUOU"
   },
   "source": [
    "fasttext_model.most_similar ('кравать') # попробовать разные опечатки"
   ],
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "[('вать', 0.7385567426681519),\n ('хавать', 0.7380610704421997),\n ('быковать', 0.7299599647521973),\n ('воровать', 0.721983790397644),\n ('пировать', 0.7135821580886841),\n ('спаивать', 0.6996143460273743),\n ('жировать', 0.6917222738265991),\n ('кать', 0.6883906126022339),\n ('стращать', 0.6878533363342285),\n ('бать', 0.6829023361206055)]"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4NBae48Eq2vX"
   },
   "source": [
    "fasttext_model.most_similar ('блять')"
   ],
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "[('бля', 0.7755434513092041),\n ('блеать', 0.7372725009918213),\n ('мля', 0.708987295627594),\n ('нахуй', 0.691758394241333),\n ('ваще', 0.6896630525588989),\n ('блин', 0.6857178211212158),\n ('ебаные', 0.675656795501709),\n ('ебаный', 0.6753869652748108),\n ('охуеть', 0.6736325621604919),\n ('сука', 0.6635270118713379)]"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ktjm7Nl9qz_q"
   },
   "source": [
    "fasttext_model.most_similar ('котэ')"
   ],
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "[('кот', 0.6769469380378723),\n ('котя', 0.6286473274230957),\n ('котик', 0.6228466033935547),\n ('котяра', 0.6106880307197571),\n ('кота', 0.6015534996986389),\n ('коты', 0.5914632678031921),\n ('котика', 0.5902719497680664),\n ('бегемотик', 0.5805137157440186),\n ('котики', 0.5798563957214355),\n ('котенок', 0.5771116018295288)]"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XeGPwlGwUyov"
   },
   "source": [
    "fasttext_model.most_similar ('некузявый')"
   ],
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "[('корявый', 0.7026479244232178),\n ('чернявый', 0.6892554759979248),\n ('писклявый', 0.6638097167015076),\n ('некрасивый', 0.6533088088035583),\n ('бестолковый', 0.6427179574966431),\n ('сопливый', 0.6411333680152893),\n ('хреновый', 0.6380565762519836),\n ('паршивый', 0.6377483606338501),\n ('махонький', 0.6357012987136841),\n ('прыщавый', 0.6353145837783813)]"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vBkb9rFHTeDa"
   },
   "source": [
    "fasttext_model.most_similar ('лебедиво')"
   ],
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "[('лебеди', 0.692702054977417),\n ('лебедин', 0.6209226250648499),\n ('диво', 0.6060458421707153),\n ('лебединое', 0.6034351587295532),\n ('лебеду', 0.588827908039093),\n ('лебеда', 0.5634397268295288),\n ('лебединой', 0.5597955584526062),\n ('лебедь', 0.5428179502487183),\n ('пеликаны', 0.5409451127052307),\n ('лебединая', 0.5382550358772278)]"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j1KtLjSxY_QP"
   },
   "source": [
    "fasttext_model.most_similar ('lol') # латиницу русская модель не делает, делает веселое"
   ],
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "[(':d', 0.682301938533783),\n ('quote', 0.6119906902313232),\n ('лол', 0.5925002694129944),\n ('ахахаха', 0.5776768326759338),\n ('facepalm', 0.576578676700592),\n ('ахаха', 0.5749382972717285),\n ('cry', 0.573516845703125),\n ('biggrin', 0.5696468353271484),\n ('p.s', 0.5673760175704956),\n ('wink', 0.5664108395576477)]"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-CbFK5nvwDB"
   },
   "source": [
    "## Как выйти за пределы слов и применить это на практике?\n",
    "\n",
    "Как я уже говорил, векторные модели сделали не для того, чтобы веселиться с векторной математикой (ну или не только для этого). В первую очередь это супер-полезный способ **векторизации текста** для практических задач комплингвистики:\n",
    "\n",
    "\n",
    "*   Классификация текстов\n",
    "*   Извлечение информации (которое часто сводится к задаче классификации слов или их последовательностей)\n",
    "*   Анализ тональности\n",
    "*   И прочее\n",
    "\n",
    "Если объектом является не отдельное слово, а предложение или текст (так бывает часто, например, при решении все той же задачи классификации текстов) то общая идея такая: весь текст превращается в вектор (набор чиселок), которые как-то зависят от векторов его слов (множества наборов чиселок). \n",
    "\n",
    "Самый очевидный вариант — сложить вектора всех слов текста или взять средний вектор всех слов текста.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C3ZVk0wtPufH"
   },
   "source": [
    "import numpy as np"
   ],
   "execution_count": 113,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QMiEuiYpmjA7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d3ecb559-f8a4-49c2-e897-bfaac7976345"
   },
   "source": [
    "fasttext_model.vectors.shape"
   ],
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "(347295, 300)"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rzj1AmjUOhRM"
   },
   "source": [
    "np.average(np.array([fasttext_model['глокая'], fasttext_model['куздра'], fasttext_model['штеко']]))"
   ],
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "0.005346726"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z19eQyiZ0od0"
   },
   "source": [
    "Чуть более тонкое есть в алгоритме doc2vec — его сделали те же люди, что и word2vec. Они придумали как бы добавлять еще одно псевдо-слово в контекст при обучении векторов слов для word2vec. В результате у нас после обучения кроме векторов слов есть еще один вектор той же размерности, который как бы побывал в контексте всех слов данного документа. Он и выдается в качестве вектора (эмбеддинга) документа. \n",
    "\n",
    "<img src = \"https://miro.medium.com/max/535/0*x-gtU4UlO8FAsRvL.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clXPsDp5OGYv"
   },
   "source": [
    "Реализация `doc2vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3Uyx_w5DS2R"
   },
   "source": [
    "\n",
    "\n",
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7SYGrASDS2S"
   },
   "source": [
    "В этом тьюториале мы постарались разобраться с тем, как работать с семантическими векторными моделями и библиотекой **gensim**. Теперь вы можете:\n",
    "* использовать готовые модели векторной семантики,осуществлять простые операции над векторами слов.\n",
    "* осуществлять предобработку текстовых данных, что может пригодиться во многих задачах обработки естественного языка;\n",
    "* тренировать векторные семантические модели. Формат моделей совместим с моделями, представленными на веб-сервисе **RusVectōrēs**;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM7jcoLqyZut"
   },
   "source": [
    "## Что мы не затронули?\n",
    "\n",
    "\n",
    "\n",
    "*   Оценка качества моделей (как тут считать точность-полноту-F-меру). См например в [этой тетрадке](https://github.com/ancatmara/data-science-nlp/blob/master/2.%20Embeddings.ipynb) у Оксаны оценку качества на задачах оценки семантической близости и поиска аналогии (Москва Россия Берлин Германия).\n",
    "*   Применение для реальных задач классификации текстов. См. в [этой тетрадке](https://github.com/mannefedov/compling_nlp_hse_course/blob/master/2020/Embeddings.ipynb) Миши Нефедова.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-O3NnAaSzJHr"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}