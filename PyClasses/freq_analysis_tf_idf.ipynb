{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня поговорим о частотном анализе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сначала установим и импортируем все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "xbvsRSBLaxkb",
    "outputId": "397681da-3ba8-40cb-fb09-201c5c1a0c52"
   },
   "outputs": [],
   "source": [
    "!pip -qq install yargy --progress-bar off\n",
    "!pip -qq install pymorphy2 --progress-bar off\n",
    "!pip -qq install rusenttokenize --progress-bar off\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "        \n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузим и прочитаем данные. Сегодня у нас тексты новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# так можно загрузить и разархивировать из  источника\n",
    "#  раскомментируйте и запустите эту ячейку, если работаете из колаба\n",
    "\n",
    "# !wget -q https://github.com/oserikov/data-science-nlp/raw/master/data/lenta_2018.txt.gz\n",
    "    \n",
    "# !gunzip lenta_2018.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используйте эту ячейку если работаете локально\n",
    "\n",
    "LENTA_TEXTS_FN = \"../data/lenta_2018_texts.txt\" # замените на релевантный путь к файлу в вашей файловой системе\n",
    "LENTA_TITLES_FH = \"../data/lenta_2018_titles.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# способ прочитать текст из файла,  вызвав питон из терминала\n",
    "\n",
    "!cat ../data/lenta_2018.txt |\\\n",
    " python3 -c \"import sys; \\\n",
    "             [print(line.strip()) for idx, line in enumerate(sys.stdin) if idx%2==1]\" \\\n",
    " > {LENTA_TEXTS_FN}\n",
    "\n",
    "!cat ../data/lenta_2018.txt |\\\n",
    " python3 -c \"import sys; \\\n",
    "             [print(line.strip()) for idx, line in enumerate(sys.stdin) if idx%2==0]\" \\\n",
    " > {LENTA_TITLES_FN}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat {LENTA_TITLES_FN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_aMbMW2ChfwW"
   },
   "source": [
    "# Частотный анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nD4QBIPflnmy"
   },
   "source": [
    "Многие компьтерные методы анализа текста основаны на статистике — в нашем случае это частотность символов / словоформ / лексем / биграмм / триграмм / частей речи и т.д., ее отношение к длине текста, средняя длина текстов и т.д.\n",
    "\n",
    "Зачем нам знать частотность слов в тексте? Например, она говорит о том, какие слова наиболее характеры для того или иного текста. Сравнивая частотные слова в разных текстах можно определить степень их близости, классифицировать по жанру, теме и т.п., а также выявить явления, характерные для языка в целом. \n",
    "\n",
    "PS:\n",
    "- Подход, когда текст представляется просто как куча слов, без информации об их порядке, называется **bag of words**.\n",
    "\n",
    "- Частотный словарь русского языка, составленный О.Н. Ляшевской и С.А. Шаровым на основе НКРЯ, можно найти [вот тут](http://dict.ruslang.ru/freq.php)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdkdjZMlgRFm"
   },
   "source": [
    "\n",
    "## Закон Ципфа\n",
    "\n",
    "**Закон Ципфа** («ранг—частота») — эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (т.н. рангу этого слова). Например, второе по используемости слово встречается примерно в два раза реже, чем первое, третье — в три раза реже, чем первое, и т.д.\n",
    "\n",
    "$$f = \\frac{a}{r}$$\n",
    " \n",
    "$f$ – частота, $r$ – ранг, $a$  – параметр, для славянских языков – около $0.07$\n",
    "\n",
    "![zipf](https://i.pics.livejournal.com/eponim2008/17443609/234916/234916_original.jpg)\n",
    "\n",
    "Закон назван именем американского лингвиста Джорджа Ципфа (правда, популяризировал он данную закономерность не для лингвистических данных, а для описания распределения экономических сил и социального статуса). \n",
    "\n",
    "**Если закон Ципфа соблюдается — значит, перед нами нормальный текст на естественном языке. Если нет, то что-то с ним не так.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOOfuVV7gTU5"
   },
   "source": [
    "## Закон Хипса\n",
    "\n",
    "**Закон Хипса** — эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины. C увеличением длины текста (количества токенов), количество *уникальных* токенов увеличивается сдедующим образом:\n",
    "\n",
    "$|V| = K*N^b$, &emsp; $N$  –  число токенов, $|V|$  – количество уникальных токенов, $K, b$  –  свободные параметры (определяются эмпирически), обычно $K \\in [10,100], b \\in [0.4, 0.6]$\n",
    "\n",
    "![heaps](http://nordbotten.com/ADM/ADM_book/figures/F4-5_Heaps.gif)\n",
    "\n",
    "**Чем больше коллекция текстов, тем меньше новых токенов появляется с её пополнением**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBa9H184WE_p",
    "tags": []
   },
   "source": [
    "# Способы считать частоту\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9RSUJ_4jm_t_"
   },
   "source": [
    "## Абсолютная частота слова\n",
    "Количество употреблений слова в тексте. Она не всегда уместна.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XE9NEoArnC5-"
   },
   "source": [
    "\n",
    "## Относительная частота слова\n",
    "это отношение его абсолютной частоты к какой-нибудь другой величине, например, к длине текста или корпуса. Существуют разные способы подсчета относительной частоты. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3grmmB0nFm-"
   },
   "source": [
    "### IPM\n",
    "Для сравнения частот в разных коллекциях текстов популярен $ipm$ *(items per million).* Как следует из названия, это отношение абсолютной частоты какого-либо элемента к объему корпуса, умноженное на миллион.\n",
    "\n",
    "$$ ipm_{word} = \\dfrac{f_{word}}V_{corpus} \\        \\times \\  1,000,000 $$ \n",
    "\n",
    "Например, если текст состоит из 500 слов, и слово \"котик\" встречается там 50 раз, то \n",
    "\n",
    "$$ ipm_{котик} = \\dfrac{50}{500} \\       \\times \\  1,000,000 \\     = 100,000 $$ \n",
    "\n",
    "Метрика IPM позволяет сравнивать тексты через их характеристики. Например, \"Я\" заметно чаще встречается в корпусе любительской литературы, чем в корпусе художественных произведений из НКРЯ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BO3bXlI7tr0A"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtuK-F8Bmbs_"
   },
   "source": [
    "Eсли мы возьмём множество документов из какой-то одной и узкой предметной области, они все будут разделять значительное количество одинаковых токенов, характерных для темы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOVnw8dAmiE7"
   },
   "source": [
    "$\\textit{Tf-Idf}$ &mdash; способ высоко оценить слова, которые одновременно\n",
    "* показательны в документе\n",
    "* не вездесущи в корпусе документов\n",
    "\n",
    "\n",
    "оценка слова будет \n",
    "* увеличиваться, если оно частотно в документе и уменьшаться\n",
    "* уменьшаться, если оно встречается во многих документах\n",
    "\n",
    "В таком противостоянии победят те слова, которые выделяют документы из множества им подобных.\n",
    "\n",
    "---\n",
    "\n",
    "$$\\textit{Tf-Idf}(\\mathit{t}, \\mathit{d}, \\mathit{c}) = \\mathit{tf}(\\mathit{t}, \\mathit{d}) \\times \\mathit{idf}(\\mathit{t}, \\mathit{c})$$\n",
    "\n",
    "$\\mathit{tf}$ &mdash; частота токена в документе\n",
    "\n",
    "$\\mathit{idf}$ &mdash; обратно монотонно частоте токена в корпусе\n",
    "\n",
    "Классическая функция $\\mathit{idf}$\n",
    "\n",
    "$$\\mathit{idf}(\\mathit{d}, \\mathit{c}) = \\log{\\frac{\\left| \\mathit{c} \\right|}{\\left| \\{ \\mathit{d} \\in \\mathit{c}: \\mathit{token} \\in \\mathit{d} \\} \\right|}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<img alt=\"r/learnmachinelearning - I made an infographic to help me remember how TF–IDF works. Hope this helps someone\"  width=\"400\"\n",
    "     src=\"https://i.redd.it/nchhwldrvt251.png\">\n",
    "\n",
    "подробнее про tf-idf можно почитать [здесь](https://ru.wikipedia.org/wiki/TF-IDF) и [здесь](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-myJ1RqZrYZE"
   },
   "source": [
    "## Облака слов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wn55OZs7riPo"
   },
   "source": [
    "Это наглядный способ посмотреть на частоты слов в тексте/корпусе: более встречаемые отображаются крупнее.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://s3.amazonaws.com/nautilus-vertical/mitp-a-170.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот пример функции, которая генерирует вордклауд из частотного словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eW41X7nArg0n"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def show_wordcloud_by_freq_dict(freq_dict):\n",
    "    \n",
    "    wordcloud = WordCloud()\n",
    "    wordcloud.generate_from_frequencies(frequencies=freq_dict)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svGaub1GpXTU"
   },
   "source": [
    "# Частотный анализ Ленты\n",
    "\n",
    "давайте проведем серию экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "qxyQT4HDpcKd",
    "outputId": "190fd64c-96dc-4925-8b5f-681653050591"
   },
   "outputs": [],
   "source": [
    "!head -1 {LENTA_TEXTS_FN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQgW46T6p7P4"
   },
   "source": [
    "### Частотный анализ Ленты максимально просто "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YbctfnDOqBK1",
    "outputId": "e4850bc8-e0fa-41f0-ff8e-351ef76c600f"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict as dd\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "with open(LENTA_TEXTS_FN) as corpus_f:\n",
    "    for document in corpus_f:\n",
    "\n",
    "        text_words_frequencies = dd(int)\n",
    "        \n",
    "        document_tokenized = document.split()\n",
    "        for word in document_tokenized:\n",
    "            text_words_frequencies[word] += 1 / len(document_tokenized)\n",
    "\n",
    "        sorted_frequency_table = sorted(text_words_frequencies.items(), \n",
    "                                        key=itemgetter(1), reverse=True)    \n",
    "        \n",
    "        for word, freq in sorted_frequency_table[:10]:\n",
    "            print('\\t'.join((word, str(freq))))\n",
    "        \n",
    "        show_wordcloud_by_freq_dict(text_words_frequencies)\n",
    "        print(document)\n",
    "        \n",
    "        proceed = input(\"proceed? ( [n] to refuse) : \")\n",
    "        if proceed.strip().lower() == \"n\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlhCElZ8qDxj"
   },
   "source": [
    "### Частотный анализ Ленты просто, но получше\n",
    "\n",
    "Теперь –– с предобработкой "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "colab_type": "code",
    "id": "-3l1VI3HtqiV",
    "outputId": "3f587006-a226-4a7a-f65d-c333c78c7cdc"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict as dd\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from rusenttokenize import ru_sent_tokenize\n",
    "import string\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "def preprocess_tokenize(text):\n",
    "    \n",
    "    text_preprocessed_tokenized = []\n",
    "        \n",
    "    for sentence in ru_sent_tokenize(text):\n",
    "        \n",
    "        clean_words = [word.strip(string.punctuation) for word in word_tokenize(text)]\n",
    "        clean_words = [word for word in clean_words if word]\n",
    "        clean_words = [word.lower() for word in clean_words if word]\n",
    "        clean_words = [word for word in clean_words if word not in russian_stopwords]\n",
    "        \n",
    "        clean_lemmas = [morph_analyzer.parse(word)[0].normal_form for word in clean_words]\n",
    "        \n",
    "        text_preprocessed_tokenized.extend(clean_lemmas)\n",
    "\n",
    "    return text_preprocessed_tokenized\n",
    "\n",
    "\n",
    "with open(LENTA_TEXTS_FN) as corpus_f:\n",
    "    for document in corpus_f:\n",
    "\n",
    "        text_words_frequencies = dd(int)\n",
    "        document_tokenized = preprocess_tokenize(document)\n",
    "        for word in document_tokenized:\n",
    "            text_words_frequencies[word] += 1 / len(document_tokenized)\n",
    "\n",
    "        sorted_frequency_table = sorted(text_words_frequencies.items(), \n",
    "                                        key=itemgetter(1), reverse=True)    \n",
    "        \n",
    "        for word, freq in sorted_frequency_table[:10]:\n",
    "            print('\\t'.join((word, str(freq))))\n",
    "        \n",
    "        show_wordcloud_by_freq_dict(text_words_frequencies)\n",
    "        print(document)\n",
    "        \n",
    "        proceed = input(\"proceed? ( [n] to refuse) : \")\n",
    "        if proceed.strip().lower() == \"n\":\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BO3bXlI7tr0A"
   },
   "source": [
    "## $\\textit{Tf-Idf}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtuK-F8Bmbs_"
   },
   "source": [
    "Если частоты считать аккуратно, то списки частотности более-менее передают контраст документов.  \n",
    "**Но!** если мы возьмём множество документов из какой-то одной и узкой предметной области -- они все будут разделять значительное количество одинаковых токенов, характерных для темы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOVnw8dAmiE7"
   },
   "source": [
    "$\\textit{Tf-Idf}$ &mdash; способ высоко оценить слова, которые одновременно\n",
    "* показательны в документе\n",
    "* не вездесущи в корпусе документов\n",
    "\n",
    "\n",
    "Наивная идея такая: давайте оценка слова будет \n",
    "* увеличиваться, если оно частотно в документе и уменьшаться\n",
    "* уменьшаться, если оно встречается во многих документах\n",
    "\n",
    "В таком противостоянии победят те слова, которые выделяют документы из кучи им подобных.\n",
    "\n",
    "---\n",
    "\n",
    "$$\\textit{Tf-Idf}(\\mathit{token}, \\mathit{document}, \\mathit{corpora}) = \\mathit{tf}(\\mathit{token}, \\mathit{document}) \\times \\mathit{idf}(\\mathit{token}, \\mathit{corpora}),$$\n",
    "\n",
    "$\\mathit{tf}$ &mdash; частота токена в документе;\n",
    "$\\mathit{idf}$ &mdash; обратно монотонно частоте токена в корпусе\n",
    "\n",
    "Классическая функция $\\mathit{idf}$ $$\\mathit{idf}(\\mathit{document}, \\mathit{corpora}) = \\log{\\frac{\\left| \\mathit{corpora} \\right|}\n",
    "{\\left| \\{ \\mathit{document} \\in \\mathit{corpora}: \\mathit{token} \\in \\mathit{document} \\} \\right|}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "подробнее про tf-idf можно почитать [здесь](https://ru.wikipedia.org/wiki/TF-IDF) и [здесь](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tv2QIxj3HNLT"
   },
   "source": [
    "### Частотный анализ Ленты с помощью tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "SjeO_F17t5zb",
    "outputId": "92146ad7-c58d-4026-d28e-c8842e772bc1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict as dd\n",
    "from operator import itemgetter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from rusenttokenize import ru_sent_tokenize\n",
    "import string\n",
    "import logging\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "def preprocess_tokenize(text):\n",
    "    \n",
    "    text_preprocessed_tokenized = []\n",
    "        \n",
    "    for sentence in ru_sent_tokenize(text):\n",
    "        \n",
    "        clean_words = [word.strip(string.punctuation) for word in word_tokenize(text)]\n",
    "        clean_words = [word for word in clean_words if word]\n",
    "        clean_words = [word.lower() for word in clean_words if word]\n",
    "        clean_words = [word for word in clean_words if word not in russian_stopwords]\n",
    "        \n",
    "        clean_lemmas = [morph_analyzer.parse(word)[0].normal_form for word in clean_words]\n",
    "        \n",
    "        text_preprocessed_tokenized.extend(clean_lemmas)\n",
    "\n",
    "    return text_preprocessed_tokenized\n",
    "\n",
    "\n",
    "corpus = []\n",
    "with open(LENTA_TEXTS_FN) as corpus_f:\n",
    "    for document in corpus_f:\n",
    "        corpus.append(document)\n",
    "        \n",
    "        if len(corpus) > 1000:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=preprocess_tokenize)\n",
    "tfidf_vectorizer.fit_transform(corpus)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "for document in corpus:\n",
    "    X = tfidf_vectorizer.transform([document])\n",
    "    \n",
    "    tfidf_scores = [(feature_names[col], X[0, col]) for col in X.nonzero()[1]]\n",
    "    freq_list = [(word, freq) for word, freq in sorted(tfidf_scores, \n",
    "                                                       key=itemgetter(1), \n",
    "                                                       reverse=True)]\n",
    "    \n",
    "    show_wordcloud_by_freq_dict(dict(freq_list))\n",
    "    print(document)\n",
    "    \n",
    "    proceed = input(\"proceed? ( [n] to refuse) : \")\n",
    "    if proceed.strip().lower() == \"n\":\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMDkOmrGbvwY"
   },
   "source": [
    "## Итог\n",
    "Так выглядит базовое извлечение ключевых слов из текстов. \n",
    "Теперь их можно использовать в дальнейших шагах ваших проектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpsYN2GfcaMN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "freq_analysis_tf-idf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-67d2a6e7",
   "language": "python",
   "display_name": "PyCharm (CompLing)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}